# Token Analysis

Analysis of token counts for the Googology Wiki statistics HTML file using different tokenization methods.

## Summary

- **File**: `data/statistics-googology-wiki-fandom.html`
- **File size**: 222.0 KB
- **Character count**: 227,335
- **Excluded content size**: 129.1 KB
- **Excluded character count**: 132,233
- **File size after exclude**: 92.9 KB
- **Character count after exclude**: 95,102
- **Analysis methods**: 3 different tokenization approaches

## Token Count Results

| Method | Tokens | Tokens (after exclude) | Processing Time | Chars/Token | Description |
|--------|--------|----------------------|----------------|-------------|-------------|
| tiktoken (GPT-4) | 91,605 | 29,395 | 0.096s | 2.48 | OpenAI GPT-4 tokenizer |
| transformers (BERT) | 109,445 | 36,072 | 0.722s | 2.08 | Hugging Face BERT tokenizer |
| Generic estimation | 75,778 | 31,700 | 0.000s | 3.00 | Character count / 3 |

---

## License and Attribution

This analysis contains content from the **Googology Wiki** (googology.fandom.com), which is licensed under the [Creative Commons Attribution-ShareAlike 3.0 Unported License](https://creativecommons.org/licenses/by-sa/3.0/).

- **Original Source**: [Googology Wiki](https://googology.fandom.com)
- **License**: [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)
- **Attribution**: Content creators and contributors of the Googology Wiki
- **Modifications**: This analysis extracts and reorganizes data from the original wiki content

*Archive fetched: 2025-07-06 01:33:43*  
*Generated by tokens.py*  
*Analysis date: 2025-07-06 07:41:08*
