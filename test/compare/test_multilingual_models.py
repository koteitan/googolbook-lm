#!/usr/bin/env python3
"""
å¤šè¨€èªå¯¾å¿œembeddingãƒ¢ãƒ‡ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆ
"""

import sys
import os
import json
import numpy as np
import torch

# Add parent directories to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel

def test_model(model_name, text):
    """æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†"""
    print(f"\n=== ãƒ¢ãƒ‡ãƒ«: {model_name} ===")
    print(f"ãƒ†ã‚­ã‚¹ãƒˆ: '{text}'")
    
    try:
        # SentenceTransformerã§ã®ãƒ†ã‚¹ãƒˆ
        print(f"\n--- SentenceTransformer ---")
        sentence_model = SentenceTransformer(model_name)
        embedding = sentence_model.encode(text, convert_to_numpy=True)
        print(f"âœ… æˆåŠŸ - Embedding shape: {embedding.shape}")
        print(f"   Embedding (first 5): {embedding[:5].tolist()}")
        
        # Tokenizerã®è©³ç´°ç¢ºèª
        print(f"\n--- Tokenizerè©³ç´° ---")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        inputs = tokenizer(text, return_tensors='pt')
        
        print(f"Token IDs: {inputs['input_ids'].tolist()[0]}")
        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
        print(f"Tokens: {tokens}")
        
        # [UNK]ã®æ•°ã‚’ãƒã‚§ãƒƒã‚¯
        unk_count = tokens.count('[UNK]')
        print(f"[UNK]ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {unk_count}")
        
        return {
            'model_name': model_name,
            'success': True,
            'embedding': embedding.tolist(),
            'token_ids': inputs['input_ids'].tolist()[0],
            'tokens': tokens,
            'unk_count': unk_count,
            'embedding_dimension': len(embedding)
        }
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'model_name': model_name,
            'success': False,
            'error': str(e)
        }

def main():
    print("=== å¤šè¨€èªå¯¾å¿œEmbeddingãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ ===")
    
    # ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
    test_text = "ã‚°ãƒ©ãƒãƒ æ•°"
    
    # ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«
    models_to_test = [
        # å…ƒã®ãƒ¢ãƒ‡ãƒ«ï¼ˆæ¯”è¼ƒç”¨ï¼‰
        'sentence-transformers/all-MiniLM-L6-v2',
        
        # å¤šè¨€èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«
        'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
        'sentence-transformers/distiluse-base-multilingual-cased',
        'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',
        
        # æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚‚ã—åˆ©ç”¨å¯èƒ½ãªã‚‰ï¼‰
        'sentence-transformers/stsb-xlm-r-multilingual',
    ]
    
    results = []
    
    for model_name in models_to_test:
        try:
            result = test_model(model_name, test_text)
            results.append(result)
            
            if result['success']:
                print(f"âœ… {model_name}: æˆåŠŸ ([UNK]: {result['unk_count']}å€‹)")
            else:
                print(f"âŒ {model_name}: å¤±æ•—")
                
        except Exception as e:
            print(f"âŒ {model_name}: ã‚¹ã‚­ãƒƒãƒ— - {e}")
            results.append({
                'model_name': model_name,
                'success': False,
                'error': str(e)
            })
    
    # çµæœã®æ¯”è¼ƒ
    print(f"\n=== çµæœæ¯”è¼ƒ ===")
    successful_results = [r for r in results if r['success']]
    
    print(f"æˆåŠŸã—ãŸãƒ¢ãƒ‡ãƒ«æ•°: {len(successful_results)}/{len(models_to_test)}")
    
    if len(successful_results) >= 2:
        print(f"\n--- [UNK]ãƒˆãƒ¼ã‚¯ãƒ³æ•°æ¯”è¼ƒ ---")
        for result in successful_results:
            status = "ğŸŸ¢ è‰¯å¥½" if result['unk_count'] == 0 else f"ğŸ”´ {result['unk_count']}å€‹"
            print(f"  {result['model_name']}: {status}")
        
        # æœ€ã‚‚UNKãŒå°‘ãªã„ãƒ¢ãƒ‡ãƒ«ã‚’æ¨å¥¨
        best_models = [r for r in successful_results if r['unk_count'] == 0]
        if best_models:
            print(f"\nğŸ¯ æ¨å¥¨ãƒ¢ãƒ‡ãƒ« ([UNK]ãªã—):")
            for model in best_models:
                print(f"  - {model['model_name']}")
                print(f"    æ¬¡å…ƒ: {model['embedding_dimension']}")
                print(f"    ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(model['tokens'])}")
        else:
            min_unk = min(r['unk_count'] for r in successful_results)
            best_models = [r for r in successful_results if r['unk_count'] == min_unk]
            print(f"\nğŸ”¶ ç›¸å¯¾çš„ã«è‰¯ã„ãƒ¢ãƒ‡ãƒ« ([UNK]: {min_unk}å€‹):")
            for model in best_models:
                print(f"  - {model['model_name']}")
                print(f"    æ¬¡å…ƒ: {model['embedding_dimension']}")
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = os.path.join(os.path.dirname(__file__), "multilingual_test_results.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nè©³ç´°çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")

if __name__ == '__main__':
    main()